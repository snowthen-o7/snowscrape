# Bot Protection Bypass Research

**Status:** Critical Priority
**Date:** 2026-02-01
**Strategy:** Tiered approach - use lightweight methods by default, escalate only when needed
**Impact:** Competitive capability with cost optimization

---

## Strategy: Smart Tiered Approach

**Core Principle:** Don't use expensive resources unless necessary.

### The Tiered System

**Tier 1 (Default) - Lightweight & Fast** âœ… *Current Implementation*
- Technology: Python `requests` + BeautifulSoup
- Cost: Minimal (~$0.01 per 1000 pages)
- Speed: 1-3 seconds per page
- Success Rate: ~60% of sites (Forever21, simple sites)
- **Use when:** Site has no bot protection

**Tier 2 (Auto-Fallback) - IP Rotation** ğŸ’¡ *Recommended Next Step*
- Technology: Same stack + residential proxy
- Cost: ~$0.50 per 1000 pages
- Speed: 2-5 seconds per page
- Success Rate: ~80% of sites
- **Use when:** Tier 1 gets 403/blocked by IP

**Tier 3 (Advanced) - Full Browser** ğŸ”§ *For Hard Targets*
- Technology: Playwright + stealth + residential proxy
- Cost: ~$5-8 per 1000 pages
- Speed: 5-15 seconds per page
- Success Rate: ~95% of sites (Costco, Amazon)
- **Use when:** Tier 2 fails, JS challenges detected

**Tier 4 (Nuclear) - Kitchen Sink** ğŸš€ *Last Resort*
- Technology: Playwright + stealth + residential proxy + CAPTCHA solving
- Cost: ~$10-15 per 1000 pages (includes CAPTCHA costs)
- Speed: 15-30 seconds per page
- Success Rate: ~99% of public sites
- **Use when:** Tier 3 fails, CAPTCHA detected

### Automatic Detection & Escalation

```python
async def smart_scrape(url, user_config):
    """
    Automatically escalate through tiers based on detection.
    """
    # Start with Tier 1 (unless user forced higher)
    tier = user_config.get('min_tier', 1)

    for attempt in range(4):  # Try up to 4 tiers
        try:
            if tier == 1:
                result = await scrape_lightweight(url)
            elif tier == 2:
                result = await scrape_with_proxy(url)
            elif tier == 3:
                result = await scrape_with_browser(url)
            elif tier == 4:
                result = await scrape_with_captcha(url)

            # Check for blocking indicators
            if detect_blocking(result):
                tier += 1
                logger.info(f"Escalating to Tier {tier}", url=url)
                continue

            # Success!
            return result, tier

        except BlockedError:
            tier += 1
            if tier > 4:
                raise ScrapingFailedError("All tiers exhausted")

    # If we get here, suggest manual review
    return None, tier

def detect_blocking(response):
    """
    Detect if we've been blocked.
    """
    indicators = [
        (403, 'status_code'),  # Forbidden
        (429, 'status_code'),  # Rate limited
        ('captcha', 'content'),  # CAPTCHA present
        ('cloudflare', 'content'),  # Cloudflare challenge
        ('access denied', 'content'),  # Generic block
        ('unusual traffic', 'content'),  # Bot detection
        ('verify you are human', 'content'),  # Human verification
    ]

    for value, check_type in indicators:
        if check_type == 'status_code' and response.status_code == value:
            return True
        elif check_type == 'content' and value.lower() in response.text.lower():
            return True

    return False
```

### User Experience Flow

**Scenario 1: Simple Site (Forever21)**
1. User enters URL in visual builder
2. System tries Tier 1 (requests)
3. âœ… Success! Returns 100 elements in 2 seconds
4. Cost: $0.01
5. User never knows tiers exist

**Scenario 2: Bot-Protected Site (Costco)**
1. User enters URL in visual builder
2. System tries Tier 1 (requests)
3. âŒ Detects: 403 Forbidden
4. System shows: "âš ï¸ Site has bot protection. Retrying with advanced methods..."
5. System tries Tier 2 (proxy)
6. âŒ Still blocked
7. System tries Tier 3 (browser)
8. âœ… Success! Returns 100 elements in 12 seconds
9. System shows: "âœ… Successfully loaded using Advanced Mode (Tier 3)"
10. Cost: $8 (user is warned about cost)

**Scenario 3: User Knows They Need Advanced**
1. User toggles "Advanced Mode" in job configuration
2. User sets: "Start with Tier 3" (browser-based)
3. System skips Tier 1/2, goes straight to browser
4. Faster for user (no failed attempts)
5. User in control of costs

### Benefits of This Approach

âœ… **Cost Optimization**
- Only pay for expensive methods when needed
- 60% of sites work with cheap Tier 1
- Average cost much lower than always using Tier 4

âœ… **Speed Optimization**
- Fast lightweight scraping for simple sites
- Only slow down when necessary

âœ… **User Control**
- Users can force a specific tier
- Users can set cost limits
- Transparent about what's happening

âœ… **Competitive Advantage**
- Still able to scrape hard targets when needed
- Better economics than competitors who always use browser

âœ… **Graceful Degradation**
- If Tier 4 fails, suggest manual review
- If budget exceeded, pause and notify user

---

## Current Situation

### What Fails

When attempting to scrape Costco with our current Python `requests` + BeautifulSoup approach:

```
403 Forbidden: This site appears to block automated access
```

**Why it fails:**
1. AWS Lambda IPs are known and blacklisted
2. HTTP client has identifiable fingerprint
3. No JavaScript execution (modern sites use JS challenges)
4. Predictable request patterns
5. Missing browser-like behavior

### What We're Up Against

Modern bot protection systems (Cloudflare, PerimeterX, Akamai, DataDome, etc.) use:

**1. IP Reputation**
- Blacklist known datacenter IPs (AWS, GCP, Azure)
- Track IP request rates and patterns
- Require residential IP addresses

**2. TLS Fingerprinting**
- Python `requests` has distinct TLS handshake
- Can identify automated tools vs real browsers
- Checks cipher suites, extensions, and order

**3. HTTP/2 Fingerprinting (JA3)**
- Unique fingerprints for HTTP clients
- Python requests library is easily identified
- Real browsers have specific fingerprints

**4. JavaScript Challenges**
- Browser fingerprinting (Canvas, WebGL, Audio)
- TLS/SSL fingerprinting at JS level
- Device motion/touch events
- Browser feature detection
- Proof-of-work challenges

**5. Behavioral Analysis**
- Mouse movement patterns
- Scroll behavior
- Click timing and patterns
- Time spent on page
- Navigation paths

**6. CAPTCHA**
- reCAPTCHA v2/v3
- hCaptcha
- Cloudflare Turnstile
- Custom challenges

---

## Implementation Architecture

> **Note:** These are tiers, not phases. Implement all tiers, but use them intelligently based on detection.

### Tier 1: Lightweight Scraping (Already Implemented) âœ…

**Current stack** - No changes needed for basic sites.

```python
def scrape_lightweight(url):
    headers = {
        'User-Agent': 'Mozilla/5.0...',
        # ... browser headers
    }
    response = requests.get(url, headers=headers, timeout=25)
    soup = BeautifulSoup(response.content, 'html.parser')
    return extract_elements(soup)
```

**When it works:** ~60% of sites without bot protection

---

### Tier 2: IP Rotation (Recommended First Implementation)

**Technology:** Current stack + residential proxy

**Cost per page:** ~$0.50 per 1000 pages
**Implementation time:** 1-2 days
**Success rate:** +20% more sites

```python
def scrape_with_proxy(url, proxy_config):
    proxies = {
        'http': proxy_config['http_proxy'],
        'https': proxy_config['https_proxy']
    }

    response = requests.get(
        url,
        headers=headers,
        proxies=proxies,
        timeout=30
    )
    return extract_elements(BeautifulSoup(response.content, 'html.parser'))
```

**Proxy Provider Options:**
- Bright Data: $500/month for 20GB (most reliable)
- Oxylabs: $300/month for 15GB (good alternative)
- SmartProxy: $225/month for 10GB (budget option)

**Recommendation:** Start with Bright Data trial, implement Tier 2 first before Tier 3.

---

### Tier 3: Headless Browser Infrastructure

**Technology:** Playwright or Puppeteer with stealth plugins

**Why this is mandatory:**
- Executes JavaScript (required for modern sites)
- Real browser fingerprint
- Can solve JS challenges automatically
- Handles dynamic content

**Implementation:**

```python
# Use Playwright with stealth mode
from playwright.async_api import async_playwright
from playwright_stealth import stealth_async

async def scrape_with_browser(url):
    async with async_playwright() as p:
        # Use real browser, not headless (headless is detectable)
        browser = await p.chromium.launch(
            headless=False,  # Some sites detect headless mode
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
            ]
        )

        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36...',
            locale='en-US',
            timezone_id='America/New_York',
        )

        page = await context.new_page()

        # Apply stealth patches to avoid detection
        await stealth_async(page)

        # Add random delays (human-like behavior)
        await page.goto(url, wait_until='networkidle')
        await page.wait_for_timeout(random.randint(2000, 5000))

        # Random mouse movements
        await page.mouse.move(random.randint(0, 1000), random.randint(0, 800))

        # Extract data
        content = await page.content()

        await browser.close()
        return content
```

**Deployment Options:**

1. **AWS Lambda + Lambda Layers**
   - Package Playwright/Chromium as Lambda layer
   - Increase timeout to 15 minutes
   - Increase memory to 3GB+
   - Cold start: 10-30 seconds

2. **AWS ECS/Fargate**
   - Better for long-running tasks
   - More memory/CPU available
   - Can run multiple browsers simultaneously
   - More expensive but more reliable

3. **Dedicated EC2 Instances**
   - Most control
   - Can use residential proxy pools
   - Best performance
   - Requires more DevOps

**Recommendation:** Start with Lambda + Layers, move to ECS if needed.

---

### Tier 4: CAPTCHA Solving

**Problem:** Even with real browsers, datacenter IPs are blocked.

**Solutions:**

#### Option A: Bright Data (Recommended)
- **Cost:** ~$500/month for 20GB
- **Pros:**
  - Largest residential proxy network
  - 72M+ IPs worldwide
  - Geo-targeting
  - Automatic rotation
  - High success rate on difficult sites
  - CAPTCHA solving included
- **Cons:**
  - Expensive
  - Pay per GB

**Integration:**
```python
proxy_config = {
    'server': 'http://brd-customer-{customer_id}-zone-{zone}:password@brd.superproxy.io:22225',
    'username': 'brd-customer-xxxxxxx-zone-residential',
    'password': 'your_password'
}

context = await browser.new_context(
    proxy=proxy_config,
    # ... other options
)
```

#### Option B: Oxylabs
- **Cost:** ~$300/month for 15GB
- **Similar to Bright Data but slightly cheaper
- Slightly smaller network

#### Option C: SmartProxy
- **Cost:** ~$225/month for 10GB
- Good for budget-conscious
- Smaller network, lower success rate on hard targets

#### Option D: Build Your Own (NOT Recommended)
- Requires managing residential IPs
- Legal/ethical concerns
- Unreliable
- More work than value

**Recommendation:** Start with Bright Data's trial, move to paid plan.

---

### Tier 3 Enhancement: Undetected Browser Techniques

**1. Undetected ChromeDriver (Python)**

```bash
pip install undetected-chromedriver
```

```python
import undetected_chromedriver as uc

driver = uc.Chrome(
    options=options,
    version_main=None,  # Auto-detect Chrome version
    use_subprocess=True
)
```

**2. Playwright Stealth Plugin**

```bash
pip install playwright-stealth
```

Patches over 30 detection points including:
- `navigator.webdriver`
- Chrome detection
- Permissions API
- Plugin inconsistencies
- Iframe contentWindow
- Hairline feature detection

**3. Browser Fingerprint Randomization**

```python
# Randomize viewport
viewport = {
    'width': random.choice([1366, 1920, 2560]),
    'height': random.choice([768, 1080, 1440])
}

# Randomize user agent (but keep it realistic)
user_agents = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36...',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36...',
    # ... rotation of real browser UAs
]

# Randomize timezone
timezones = ['America/New_York', 'America/Chicago', 'America/Los_Angeles']

context = await browser.new_context(
    viewport=viewport,
    user_agent=random.choice(user_agents),
    timezone_id=random.choice(timezones),
    locale='en-US',
    permissions=['geolocation'],
    geolocation={'latitude': 40.7128, 'longitude': -74.0060},  # NYC
    color_scheme='light',
    device_scale_factor=1,
)
```

---

### Phase 4: CAPTCHA Solving (Required for Some Sites)

**Problem:** Sites like Costco may show CAPTCHA challenges.

**Solutions:**

#### Option A: 2Captcha (Recommended)
- **Cost:** ~$3 per 1000 CAPTCHAs
- Supports: reCAPTCHA v2/v3, hCaptcha, Cloudflare Turnstile
- 95%+ success rate
- 10-30 second solve time

**Integration:**
```python
from twocaptcha import TwoCaptcha

solver = TwoCaptcha('YOUR_API_KEY')

# For reCAPTCHA
result = solver.recaptcha(
    sitekey='6Le-wvkSAAAAAPBMRTvw0Q4Muexq9bi0DJwx_mJ-',
    url='https://www.costco.com'
)

# Inject solution
await page.evaluate(f'document.getElementById("g-recaptcha-response").innerHTML="{result["code"]}";')
```

#### Option B: Anti-Captcha
- Similar pricing and features
- Slightly faster (8-25 seconds)
- Good alternative if 2Captcha has issues

#### Option C: CapSolver
- AI-powered solving
- Best for difficult CAPTCHAs
- More expensive (~$5 per 1000)

**Recommendation:** Start with 2Captcha (cheapest and reliable).

---

## User Interface & Controls

### Job Configuration UI

**Advanced Scraping Options** (Collapsible Section in Job Creation)

```typescript
interface ScraperConfig {
  // Tier Selection
  scraping_mode: 'auto' | 'lightweight' | 'proxy' | 'browser' | 'advanced';

  // Auto-escalation settings
  auto_escalate: boolean;  // Default: true
  max_tier: 1 | 2 | 3 | 4;  // Default: 4 (allow all)

  // Cost controls
  cost_limit_per_page: number;  // Default: $0.10
  notify_on_escalation: boolean;  // Default: true

  // Proxy settings (Tier 2+)
  proxy_provider?: 'brightdata' | 'oxylabs' | 'smartproxy';
  proxy_geo?: string;  // 'us', 'uk', 'global', etc.

  // Browser settings (Tier 3+)
  headless: boolean;  // Default: false (more stealth)
  stealth_mode: boolean;  // Default: true

  // CAPTCHA settings (Tier 4)
  solve_captchas: boolean;  // Default: false (user opt-in)
  captcha_budget: number;  // Max $ to spend on CAPTCHAs
}
```

**UI Mockup:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Create New Scraping Job                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚ Target URL                                              â”‚
â”‚ [https://www.example.com/products        ]             â”‚
â”‚                                                          â”‚
â”‚ â–¼ Advanced Scraping Options                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚ Scraping Mode: â—‹ Auto (Recommended)              â”‚ â”‚
â”‚   â”‚                â—‹ Lightweight Only (Fastest)      â”‚ â”‚
â”‚   â”‚                â—‹ Force Browser (Slow but Reliable)â”‚ â”‚
â”‚   â”‚                                                   â”‚ â”‚
â”‚   â”‚ â˜‘ Auto-escalate if blocked                      â”‚ â”‚
â”‚   â”‚   â””â”€ Max Tier: [Tier 4 (All methods) â–¼]        â”‚ â”‚
â”‚   â”‚                                                   â”‚ â”‚
â”‚   â”‚ Cost Control                                      â”‚ â”‚
â”‚   â”‚ Max cost per page: [$0.10      ]                â”‚ â”‚
â”‚   â”‚ â˜‘ Notify me when using expensive methods        â”‚ â”‚
â”‚   â”‚                                                   â”‚ â”‚
â”‚   â”‚ â˜ Enable CAPTCHA solving (Tier 4)               â”‚ â”‚
â”‚   â”‚   â””â”€ Max CAPTCHA budget: [$50.00    ]          â”‚ â”‚
â”‚   â”‚       Estimated: ~16,000 CAPTCHAs                â”‚ â”‚
â”‚   â”‚                                                   â”‚ â”‚
â”‚   â”‚ [Show cost estimates]                            â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Visual Builder Integration

**When user clicks "Load Page":**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [âŸ³ Loading...] Fetching page structure                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â†“ (2 seconds)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [âš ï¸] Site has bot protection (403 Forbidden)            â”‚
â”‚                                                          â”‚
â”‚ Retrying with Tier 2 (Residential Proxy)...            â”‚
â”‚ Estimated cost: $0.005                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â†“ (3 seconds)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [âš ï¸] Still blocked. Escalating to Tier 3                â”‚
â”‚                                                          â”‚
â”‚ Using headless browser with stealth mode...             â”‚
â”‚ This may take 10-15 seconds                             â”‚
â”‚ Estimated cost: $0.08                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â†“ (12 seconds)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [âœ…] Successfully loaded using Tier 3 (Browser Mode)    â”‚
â”‚                                                          â”‚
â”‚ Loaded 100 elements from page                           â”‚
â”‚ Cost: $0.08 | Time: 17 seconds                          â”‚
â”‚                                                          â”‚
â”‚ â„¹ï¸ This site requires Tier 3. Consider setting         â”‚
â”‚    "Start with Browser" for faster future runs.        â”‚
â”‚                                                          â”‚
â”‚ [Set as default for this domain]                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Real-Time Monitoring Dashboard

**Job Details Page - Tier Usage Stats**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Job: Costco Product Scraper                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Status: Running (47/100 pages)                          â”‚
â”‚                                                          â”‚
â”‚ Tier Usage:                                             â”‚
â”‚ â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘ Tier 1: 2% (2 pages)   $0.02              â”‚
â”‚ â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘ Tier 2: 5% (5 pages)   $2.50              â”‚
â”‚ â–“â–“â–“â–“â–“â–“â–“â–“â–“â–‘ Tier 3: 85% (40 pages) $320.00            â”‚
â”‚ â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ Tier 4: 8% (3 pages)   $45.00             â”‚
â”‚                                                          â”‚
â”‚ Total Cost So Far: $367.52                              â”‚
â”‚ Estimated Final: $780.00                                â”‚
â”‚                                                          â”‚
â”‚ âš ï¸ High Tier 3 usage detected                           â”‚
â”‚    Recommendation: Set "Force Tier 3" to skip failed    â”‚
â”‚    Tier 1/2 attempts and save time.                     â”‚
â”‚                                                          â”‚
â”‚ [Apply recommendation]  [View detailed logs]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Notification Examples

**Email/In-App Notifications:**

```
ğŸ”” Job Escalation Alert

Your job "Costco Products" is using Tier 3 (Browser Mode).

Current cost: $45.23 (15 pages)
Estimated total: $301.53 (100 pages)

This is higher than your configured limit of $100.

Actions:
â€¢ Pause job and adjust settings
â€¢ Continue with current settings
â€¢ Increase budget to $350
```

```
âœ… Job Optimization Tip

Your job "Target Electronics" successfully ran using:
- Tier 1: 95% of pages
- Tier 2: 5% of pages

Total cost: $5.23 for 1000 pages
Average cost: $0.005 per page

This is great! Target works well with lightweight scraping.
```

### Domain Intelligence

**Automatic learning and suggestions:**

```python
class DomainIntelligence:
    """
    Learn which tier works best for each domain.
    """
    def __init__(self):
        self.domain_stats = {}  # domain -> {tier: success_rate}

    def record_attempt(self, domain, tier, success):
        """Record tier success for a domain."""
        if domain not in self.domain_stats:
            self.domain_stats[domain] = {1: [], 2: [], 3: [], 4: []}

        self.domain_stats[domain][tier].append(success)

    def recommend_tier(self, domain):
        """Recommend starting tier for a domain."""
        if domain not in self.domain_stats:
            return 1  # Default to cheapest

        # Find tier with best success rate
        best_tier = 1
        best_rate = 0

        for tier, attempts in self.domain_stats[domain].items():
            if len(attempts) >= 3:  # Need at least 3 attempts
                success_rate = sum(attempts) / len(attempts)
                if success_rate > best_rate:
                    best_rate = success_rate
                    best_tier = tier

        return best_tier if best_rate > 0.8 else 1
```

**UI Integration:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Target URL: https://www.costco.com/products             â”‚
â”‚                                                          â”‚
â”‚ ğŸ’¡ Based on 47 previous scrapes of costco.com:          â”‚
â”‚    â€¢ Tier 1 success rate: 2%                            â”‚
â”‚    â€¢ Tier 3 success rate: 98%                           â”‚
â”‚                                                          â”‚
â”‚    Recommendation: Start with Tier 3 (Browser Mode)     â”‚
â”‚    This will save ~15 seconds per page.                 â”‚
â”‚                                                          â”‚
â”‚    [Apply recommendation]  [Start with Tier 1 anyway]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Supporting Features: Request Rate Limiting & Session Management

**1. Smart Rate Limiting**

```python
import random
import time

class RateLimiter:
    def __init__(self):
        self.last_request = 0
        self.requests_this_minute = 0

    async def wait(self):
        # Random delay between 2-8 seconds (human-like)
        delay = random.uniform(2, 8)

        # Add extra delay every 10 requests (simulate user behavior)
        if self.requests_this_minute % 10 == 0:
            delay += random.uniform(30, 120)  # Take a "break"

        await asyncio.sleep(delay)
        self.requests_this_minute += 1
```

**2. Session Persistence**

```python
# Save cookies and reuse sessions (like a real user)
await context.storage_state(path='session.json')

# Later, restore session
context = await browser.new_context(
    storage_state='session.json'
)
```

**3. Distributed Requests**

- Spread requests across multiple IPs
- Use different browser instances
- Vary request patterns

---

### Phase 6: Monitoring & Fallback Strategies

**1. Detection Monitoring**

```python
async def check_for_blocking(page):
    content = await page.content()

    # Check for common blocking patterns
    blocking_indicators = [
        'access denied',
        'captcha',
        'cloudflare',
        'unusual traffic',
        'please verify',
        'robot',
        'automated'
    ]

    if any(indicator in content.lower() for indicator in blocking_indicators):
        logger.warning("Potential blocking detected", url=page.url)
        return True

    return False
```

**2. Automatic Fallback**

```python
async def scrape_with_fallback(url, max_retries=3):
    for attempt in range(max_retries):
        try:
            # Try method 1: Stealth browser + residential proxy
            result = await scrape_with_browser(url, proxy='residential')

            if not await check_for_blocking(result):
                return result

        except Exception as e:
            logger.warning(f"Attempt {attempt + 1} failed", error=str(e))

            # Fallback to different approach
            if attempt == 0:
                # Try different proxy
                result = await scrape_with_browser(url, proxy='mobile_residential')
            elif attempt == 1:
                # Add CAPTCHA solving
                result = await scrape_with_captcha_solving(url)
            else:
                # Final attempt: Manual intervention queue
                await queue_for_manual_review(url)

        await asyncio.sleep(60 * (attempt + 1))  # Exponential backoff

    raise ScrapingFailedError("All retry attempts failed")
```

---

## Recommended Implementation Plan

**Strategy:** Implement tiers incrementally, test and deploy each before moving to next.

### Week 1: Tier 1 Enhancement & Tier 2 (Proxy)
**Goal:** Handle 80% of sites with low cost

1. **Enhance Tier 1 detection**
   - Implement `detect_blocking()` function
   - Add status code and content pattern matching
   - Log blocking indicators

2. **Implement Tier 2 (Residential Proxy)**
   - Sign up for Bright Data trial (or SmartProxy for budget)
   - Add proxy configuration to backend
   - Integrate proxy rotation logic
   - Test on moderately protected sites (Walmart, Target)

3. **Frontend: Basic tier controls**
   - Add "Scraping Mode" dropdown in job creation
   - Add escalation notifications in visual builder
   - Show tier used in job results

**Success Criteria:**
- âœ… Tier 1 â†’ Tier 2 escalation working
- âœ… 80%+ success rate on moderate sites
- âœ… Cost: $0.005 per page average

### Week 2: Tier 3 (Browser) Foundation
**Goal:** Handle hard targets like Costco

4. **Set up Playwright infrastructure**
   - Create Lambda layer with Chromium
   - Set up undetected browser config
   - Integrate stealth plugins
   - Test on Costco

5. **Frontend: Advanced controls**
   - Add "Force Browser Mode" option
   - Add real-time escalation notifications
   - Show tier breakdown in job monitoring

**Success Criteria:**
- âœ… Tier 3 working on Costco
- âœ… 50%+ success rate on hard sites
- âœ… Sub-20 second scrape time

### Week 3: Tier 4 (CAPTCHA) & Optimization
**Goal:** Handle the hardest sites, optimize costs

6. **Implement Tier 4 (CAPTCHA Solving)**
   - Integrate 2Captcha API
   - Automatic CAPTCHA detection
   - Cost tracking and budget limits

7. **Domain Intelligence**
   - Build domain learning system
   - Store tier success rates per domain
   - Auto-suggest best tier for known domains

8. **Cost Optimization**
   - Implement session caching
   - Add request rate limiting
   - Smart retry strategies

**Success Criteria:**
- âœ… CAPTCHA solving working
- âœ… 95%+ success on all targets
- âœ… Average cost under $0.05 per page

### Week 4: Production & Monitoring
**Goal:** Full production deployment with monitoring

9. **Monitoring Dashboard**
   - Tier usage statistics per job
   - Real-time cost tracking
   - Success rate by domain

10. **User Experience Polish**
    - Email notifications for escalation
    - Cost warning alerts
    - Optimization recommendations

11. **Load Testing**
    - Test with 1000+ page jobs
    - Validate cost projections
    - Ensure Lambda/ECS stability

**Success Criteria:**
- âœ… Full monitoring in place
- âœ… Users can control tiers easily
- âœ… System automatically optimizes

### Post-Launch: Continuous Improvement

12. **Expand proxy providers** (if needed)
    - Add Oxylabs as fallback
    - Add SmartProxy for budget users

13. **Advanced features**
    - Session persistence
    - Behavioral simulation refinement
    - ML-based blocking detection

---

## Cost Estimation

**Key Insight:** With tiered approach, actual costs are much lower than max costs.

### Tiered Cost Model (100K pages/month)

#### Scenario 1: Mixed Sites (60% simple, 30% moderate, 10% hard)

| Tier | Pages | Cost/Page | Subtotal |
|------|-------|-----------|----------|
| Tier 1 (requests) | 60,000 | $0.0001 | $6 |
| Tier 2 (proxy) | 30,000 | $0.005 | $150 |
| Tier 3 (browser) | 9,000 | $0.08 | $720 |
| Tier 4 (CAPTCHA) | 1,000 | $0.15 | $150 |
| **Total** | **100,000** | | **$1,026** |

**Average cost per page:** $0.01

#### Scenario 2: Mostly Simple Sites (90% simple, 8% moderate, 2% hard)

| Tier | Pages | Cost/Page | Subtotal |
|------|-------|-----------|----------|
| Tier 1 | 90,000 | $0.0001 | $9 |
| Tier 2 | 8,000 | $0.005 | $40 |
| Tier 3 | 1,800 | $0.08 | $144 |
| Tier 4 | 200 | $0.15 | $30 |
| **Total** | **100,000** | | **$223** |

**Average cost per page:** $0.002

#### Scenario 3: Mostly Hard Sites (10% simple, 30% moderate, 60% hard)

| Tier | Pages | Cost/Page | Subtotal |
|------|-------|-----------|----------|
| Tier 1 | 10,000 | $0.0001 | $1 |
| Tier 2 | 30,000 | $0.005 | $150 |
| Tier 3 | 54,000 | $0.08 | $4,320 |
| Tier 4 | 6,000 | $0.15 | $900 |
| **Total** | **100,000** | | **$5,371** |

**Average cost per page:** $0.05

### Infrastructure Costs

| Component | Monthly Cost |
|-----------|--------------|
| Residential Proxies (Bright Data) | $500 (included in per-page costs) |
| AWS Lambda (increased resources) | $100-200 |
| Monitoring & Storage | $50 |
| **Base Infrastructure** | **$150-250/month** |

### Comparison to "Always Browser" Approach

If you always used Tier 3 for everything:
- 100K pages Ã— $0.08 = $8,000/month
- With tiered approach: $223-5,371/month depending on mix
- **Savings: 60-97% depending on site difficulty distribution**

### Low Volume (<10K pages/month)

- Mostly simple sites: $25-50/month
- Mixed sites: $100-200/month
- Mostly hard sites: $500-800/month

### High Volume (>1M pages/month)

- Mostly simple: $2,000-3,000/month
- Mixed: $10,000-15,000/month
- Mostly hard: $50,000+/month (consider negotiating volume discounts)

---

## Competitive Analysis

### What Competitors Use

**ScrapingBee, Oxylabs, Bright Data:**
- Headless browsers (Playwright/Puppeteer)
- Residential proxy networks
- Automatic CAPTCHA solving
- AI-powered undetection
- **Pricing:** $50-500/month for similar volumes

**Why they're ahead:**
- Years of anti-detection R&D
- Large residential proxy networks
- Economies of scale
- Dedicated infrastructure

**How we catch up:**
- Use same technologies (Playwright + proxies)
- Leverage existing CAPTCHA services
- Start with hard targets as differentiator
- Offer better UX/integration

---

## Success Metrics

### Phase 1 Goals (MVP)
- âœ… 80%+ success rate on moderate sites (Target, Walmart)
- âœ… 50%+ success rate on hard sites (Costco, Amazon)
- âœ… <30 second average scrape time
- âœ… CAPTCHA solving when needed

### Phase 2 Goals (Production)
- âœ… 95%+ success rate on moderate sites
- âœ… 80%+ success rate on hard sites
- âœ… <15 second average scrape time
- âœ… Automatic fallback working
- âœ… Cost under $1,000/month for 100K pages

### Phase 3 Goals (Advanced)
- âœ… 98%+ success rate on all public sites
- âœ… Handle rate limiting automatically
- âœ… Session persistence working
- âœ… Sub-10 second scrape time

---

## Technical Risks & Mitigations

### Risk 1: Lambda Cold Starts
**Impact:** 10-30 second delay on first request
**Mitigation:**
- Keep Lambda warm with CloudWatch Events
- Consider ECS for consistent performance
- Batch requests where possible

### Risk 2: Proxy IP Bans
**Impact:** Temporary inability to scrape certain sites
**Mitigation:**
- Multiple proxy providers
- Automatic provider switching
- IP rotation strategies
- Fallback to manual review

### Risk 3: CAPTCHA Cost Explosion
**Impact:** Unexpected $500+ CAPTCHA bills
**Mitigation:**
- Set spending limits on 2Captcha
- Better bot detection avoidance (fewer CAPTCHAs)
- Cache successful sessions
- Rate limiting

### Risk 4: Site Changes
**Impact:** Selectors break, scraping fails
**Mitigation:**
- Monitoring and alerts
- Automatic fallback to visual builder
- Version control for selectors
- Community-maintained templates

---

## Legal & Ethical Considerations

### Legal
- âœ… Scraping public data is generally legal (US case law)
- âŒ Don't bypass authentication/paywalls
- âŒ Respect `robots.txt` (configurable)
- âœ… Use rate limiting to avoid DoS
- âœ… Provide clear terms of service

### Ethical
- âœ… Be respectful of target sites
- âœ… Don't cause harm or outages
- âœ… Cache aggressively to reduce load
- âœ… Provide value to end users
- âŒ Don't scrape personal data without consent

### Terms of Service
- Some sites explicitly prohibit scraping
- Users must agree to use responsibly
- Provide warnings for certain targets
- Consider "use at your own risk" disclaimer

---

## Next Steps

### Immediate (This Week)
1. Research Bright Data trial signup
2. Set up Playwright Lambda layer
3. Implement basic stealth techniques
4. Test on Costco

### Short Term (This Month)
5. Full residential proxy integration
6. CAPTCHA solving implementation
7. Test on 10+ difficult sites
8. Document success rates

### Long Term (Next Quarter)
9. Build monitoring dashboard
10. Optimize costs
11. Add more proxy providers
12. Scale to high volume

---

## References & Resources

### Documentation
- Playwright: https://playwright.dev/python/
- Playwright Stealth: https://github.com/AtuboDad/playwright_stealth
- Undetected ChromeDriver: https://github.com/ultrafunkamsterdam/undetected-chromedriver
- 2Captcha API: https://2captcha.com/2captcha-api

### Proxy Providers
- Bright Data: https://brightdata.com/
- Oxylabs: https://oxylabs.io/
- SmartProxy: https://smartproxy.com/

### Bot Detection Info
- Cloudflare Bot Detection: https://developers.cloudflare.com/bots/
- PerimeterX: https://www.humansecurity.com/products/perimeter-x
- DataDome: https://datadome.co/

### Case Studies
- "Bypassing Cloudflare Bot Protection" (2024)
- "Residential Proxies vs Bot Detection" (2025)
- "CAPTCHA Solving ROI Analysis" (2025)

---

**Last Updated:** 2026-02-01
**Next Review:** When implementation begins
**Owner:** Engineering Team
